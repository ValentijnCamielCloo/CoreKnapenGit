# License: Apache 2.0. See LICENSE file in root directory.
# Copyright(c) 2015-2017 Intel Corporation. All Rights Reserved.

import math
import time
import cv2
import numpy as np
import pyrealsense2 as rs

class AppState:
    def __init__(self, *args, **kwargs):
        self.WIN_NAME = 'RealSense'
        self.pitch, self.yaw = math.radians(-10), math.radians(-15)
        self.translation = np.array([0, 0, -1], dtype=np.float32)
        self.distance = 1
        self.prev_mouse = 0, 0
        self.mouse_btns = [False, False, False]
        self.paused = False
        self.decimate = 1
        self.scale = True
        self.color = True

    def reset(self):
        self.pitch, self.yaw, self.distance = 0, 0, 2
        self.translation[:] = 0, 0, -1

    @property
    def rotation(self):
        Rx, _ = cv2.Rodrigues((self.pitch, 0, 0))
        Ry, _ = cv2.Rodrigues((0, self.yaw, 0))
        return np.dot(Ry, Rx).astype(np.float32)

    @property
    def pivot(self):
        return self.translation + np.array((0, 0, self.distance), dtype=np.float32)

state = AppState()

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()

pipeline_wrapper = rs.pipeline_wrapper(pipeline)
pipeline_profile = config.resolve(pipeline_wrapper)
device = pipeline_profile.get_device()

found_rgb = False
for s in device.sensors:
    if s.get_info(rs.camera_info.name) == 'RGB Camera':
        found_rgb = True
        break
if not found_rgb:
    print("The demo requires Depth camera with Color sensor")
    exit(0)

config.enable_stream(rs.stream.depth, rs.format.z16, 30)
config.enable_stream(rs.stream.color, rs.format.bgr8, 30)

# Start streaming
pipeline.start(config)

# Get stream profile and camera intrinsics
profile = pipeline.get_active_profile()
depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))
depth_intrinsics = depth_profile.get_intrinsics()
w, h = depth_intrinsics.width, depth_intrinsics.height

# Processing blocks
pc = rs.pointcloud()
decimate = rs.decimation_filter()
decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)
colorizer = rs.colorizer()

# Distance thresholds in meters
min_distance_threshold = 1.0
max_distance_threshold = 2.0

def calculate_distance_stats(vertices):
    distances = np.linalg.norm(vertices, axis=1)
    if distances.size == 0:
        return float('inf'), float('-inf')
    return np.min(distances), np.max(distances)

def project(vertices):
    """ A simple projection function for the 3D points to 2D. 
        Assumes a simple perspective projection for demonstration purposes. 
        Adjust this function based on your actual projection requirements.
    """
    # Dummy projection logic (for illustration)
    # Replace with actual projection logic as needed
    focal_length = 500  # Placeholder value
    x = (vertices[:, 0] * focal_length / vertices[:, 2]) + (w / 2)
    y = (vertices[:, 1] * focal_length / vertices[:, 2]) + (h / 2)
    return np.stack((x, y), axis=-1)

while True:
    if not state.paused:
        frames = pipeline.wait_for_frames()

        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()
        depth_frame = decimate.process(depth_frame)

        # Generate point cloud and texture
        points = pc.calculate(depth_frame)
        vertices = np.asanyarray(points.get_vertices()).view(np.float32).reshape(-1, 3)

        # Calculate distance statistics
        closest, farthest = calculate_distance_stats(vertices)
        print(f"Closest point distance: {closest:.2f} m, Farthest point distance: {farthest:.2f} m")

        # Filter points based on distance (1m to 2m)
        distances = np.linalg.norm(vertices, axis=1)
        mask = (distances >= min_distance_threshold) & (distances <= max_distance_threshold)
        filtered_vertices = vertices[mask]

        # Display counts in the terminal
        print(f"Initial points: {vertices.shape[0]}, Filtered points: {filtered_vertices.shape[0]}")

        # Prepare point cloud data for rendering
        out = np.zeros((h, w, 3), dtype=np.uint8)
        if state.color:
            tex_coords = np.asanyarray(points.get_texture_coordinates()).view(np.float32).reshape(-1, 2)[mask]
            texture = cv2.cvtColor(np.asanyarray(color_frame.get_data()), cv2.COLOR_BGR2RGB)
            for i in range(len(filtered_vertices)):
                x, y = project(filtered_vertices[i:i + 1]).reshape(2)
                if not (np.isnan(x) or np.isnan(y)):
                    x, y = int(x), int(y)
                    if 0 <= x < w and 0 <= y < h:
                        out[y, x] = texture[int(tex_coords[i][1] * h), int(tex_coords[i][0] * w)]
        else:
            for i in range(len(filtered_vertices)):
                x, y = project(filtered_vertices[i:i + 1]).reshape(2)
                if not (np.isnan(x) or np.isnan(y)):
                    x, y = int(x), int(y)
                    if 0 <= x < w and 0 <= y < h:
                        out[y, x] = (0xff, 0xff, 0xff)

        # Draw additional visuals
        axes(out, state.pivot, state.rotation)
        frustum(out, depth_intrinsics)
        grid(out, state.pivot, state.rotation)

        cv2.imshow(state.WIN_NAME, out)

        key = cv2.waitKey(1)
        if key == ord('q') or key == 27:
            break
        if key == ord('p'):
            state.paused = not state.paused
        if key == ord('r'):
            state.reset()
        if key == ord('d'):
            state.decimate = (state.decimate + 1) % 6
            decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)
        if key == ord('z'):
            state.scale = not state.scale
        if key == ord('c'):
            state.color = not state.color

# Stop streaming
pipeline.stop()
